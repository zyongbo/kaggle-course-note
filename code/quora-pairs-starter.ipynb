{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T13:45:50.773220Z",
     "iopub.status.busy": "2021-04-19T13:45:50.772709Z",
     "iopub.status.idle": "2021-04-19T13:45:51.557504Z",
     "shell.execute_reply": "2021-04-19T13:45:51.556562Z",
     "shell.execute_reply.started": "2021-04-19T13:45:50.773172Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "INPUT_PATH = '/home/lyz/work/kaggle/kaggle-quora-question-pairs/'\n",
    "\n",
    "# 特征工程 + 树模型\n",
    "# 词向量 + 孪生网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T13:46:12.066863Z",
     "iopub.status.busy": "2021-04-19T13:46:12.066292Z",
     "iopub.status.idle": "2021-04-19T13:46:12.093598Z",
     "shell.execute_reply": "2021-04-19T13:46:12.092794Z",
     "shell.execute_reply.started": "2021-04-19T13:46:12.066816Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(INPUT_PATH + 'train.csv', nrows=5000)\n",
    "df_test  = pd.read_csv(INPUT_PATH + 'test.csv', nrows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T13:46:12.581643Z",
     "iopub.status.busy": "2021-04-19T13:46:12.581105Z",
     "iopub.status.idle": "2021-04-19T13:46:12.614575Z",
     "shell.execute_reply": "2021-04-19T13:46:12.613621Z",
     "shell.execute_reply.started": "2021-04-19T13:46:12.581595Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idf\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    return 0 if count < min_count else 1 / (count + eps)\n",
    "\n",
    "\n",
    "train_qs = pd.Series(\n",
    "    df_train['question1'].tolist() + df_train['question2'].tolist()\n",
    ").astype(str)\n",
    "\n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T13:46:12.988111Z",
     "iopub.status.busy": "2021-04-19T13:46:12.987551Z",
     "iopub.status.idle": "2021-04-19T13:46:13.000484Z",
     "shell.execute_reply": "2021-04-19T13:46:12.999834Z",
     "shell.execute_reply.started": "2021-04-19T13:46:12.988062Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def word_shares(row):\n",
    "    \n",
    "    # 第1种情况：句子1只包含停用词\n",
    "    q1_list = str(row['question1']).lower().split()\n",
    "    q1 = set(q1_list)\n",
    "    q1words = q1.difference(stops)\n",
    "    if len(q1words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "    \n",
    "    # 第2种情况：句子2只包含停用词\n",
    "    q2_list = str(row['question2']).lower().split()\n",
    "    q2 = set(q2_list)\n",
    "    q2words = q2.difference(stops)\n",
    "    if len(q2words) == 0:\n",
    "        return '0:0:0:0:0:0:0:0'\n",
    "\n",
    "    \n",
    "    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max( len(q1_list), len(q2_list) )\n",
    "    q1stops = q1.intersection(stops)\n",
    "    q2stops = q2.intersection(stops)\n",
    "\n",
    "    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n",
    "    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n",
    "\n",
    "    shared_2gram = q1_2gram.intersection(q2_2gram)\n",
    "\n",
    "    shared_words = q1words.intersection(q2words)\n",
    "    shared_weights = [weights.get(w, 0) for w in shared_words]\n",
    "    q1_weights = [weights.get(w, 0) for w in q1words]\n",
    "    q2_weights = [weights.get(w, 0) for w in q2words]\n",
    "    total_weights = q1_weights + q2_weights\n",
    "    \n",
    "    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n",
    "    R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n",
    "    R31 = len(q1stops) / len(q1words) #stops in q1\n",
    "    R32 = len(q2stops) / len(q2words) #stops in q2\n",
    "    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n",
    "    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n",
    "    if len(q1_2gram) + len(q2_2gram) == 0:\n",
    "        R2gram = 0\n",
    "    else:\n",
    "        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n",
    "    return '{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T13:46:13.498419Z",
     "iopub.status.busy": "2021-04-19T13:46:13.497882Z",
     "iopub.status.idle": "2021-04-19T13:46:14.420303Z",
     "shell.execute_reply": "2021-04-19T13:46:14.419800Z",
     "shell.execute_reply.started": "2021-04-19T13:46:13.498372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_train, df_test])\n",
    "df['word_shares'] = df.apply(word_shares, axis=1)\n",
    "\n",
    "train_test = pd.DataFrame()\n",
    "\n",
    "train_test['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n",
    "train_test['word_match_2root'] = np.sqrt(train_test['word_match'])\n",
    "train_test['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n",
    "train_test['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n",
    "\n",
    "train_test['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n",
    "train_test['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n",
    "train_test['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\n",
    "train_test['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\n",
    "train_test['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\n",
    "train_test['diff_stops_r']     = train_test['stops1_ratio'] - train_test['stops2_ratio']\n",
    "\n",
    "train_test['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\n",
    "train_test['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\n",
    "train_test['diff_len'] = train_test['len_q1'] - train_test['len_q2']\n",
    "\n",
    "train_test['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "train_test['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\n",
    "train_test['diff_caps'] = train_test['caps_count_q1'] - train_test['caps_count_q2']\n",
    "\n",
    "train_test['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "train_test['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\n",
    "train_test['diff_len_char'] = train_test['len_char_q1'] - train_test['len_char_q2']\n",
    "\n",
    "train_test['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "train_test['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "train_test['diff_len_word'] = train_test['len_word_q1'] - train_test['len_word_q2']\n",
    "\n",
    "train_test['avg_world_len1'] = train_test['len_char_q1'] / train_test['len_word_q1']\n",
    "train_test['avg_world_len2'] = train_test['len_char_q2'] / train_test['len_word_q2']\n",
    "train_test['diff_avg_word'] = train_test['avg_world_len1'] - train_test['avg_world_len2']\n",
    "\n",
    "train_test['exactly_same'] = (df['question1'] == df['question2']).astype(int)\n",
    "train_test['duplicated'] = df.duplicated(['question1','question2']).astype(int)\n",
    "\n",
    "def add_word_count(x, df, word):\n",
    "    x['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n",
    "    x[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n",
    "    \n",
    "add_word_count(train_test, df, 'how')\n",
    "add_word_count(train_test, df, 'what')\n",
    "add_word_count(train_test, df, 'which')\n",
    "add_word_count(train_test, df, 'who')\n",
    "add_word_count(train_test, df, 'where')\n",
    "add_word_count(train_test, df, 'when')\n",
    "add_word_count(train_test, df, 'why')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T13:46:14.421256Z",
     "iopub.status.busy": "2021-04-19T13:46:14.421086Z",
     "iopub.status.idle": "2021-04-19T13:46:15.136716Z",
     "shell.execute_reply": "2021-04-19T13:46:15.136179Z",
     "shell.execute_reply.started": "2021-04-19T13:46:14.421241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.654580</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.656680</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.622795</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.626756</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.595998</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.601964</td>\n",
       "      <td>0.002194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.573223</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>0.581153</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.553685</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.563505</td>\n",
       "      <td>0.003030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.536679</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.548681</td>\n",
       "      <td>0.002879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.521496</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.535619</td>\n",
       "      <td>0.003470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.508277</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.524311</td>\n",
       "      <td>0.003450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.496489</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.514817</td>\n",
       "      <td>0.003736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.486039</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.506007</td>\n",
       "      <td>0.004347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.476188</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.498485</td>\n",
       "      <td>0.004358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.467736</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.492051</td>\n",
       "      <td>0.004494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.459941</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.485878</td>\n",
       "      <td>0.004730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.452733</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.480734</td>\n",
       "      <td>0.005268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.446083</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.476177</td>\n",
       "      <td>0.005645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.472176</td>\n",
       "      <td>0.005912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.434987</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.468421</td>\n",
       "      <td>0.005918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.430151</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.465186</td>\n",
       "      <td>0.006108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.425851</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>0.462419</td>\n",
       "      <td>0.006131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.421787</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.460107</td>\n",
       "      <td>0.006218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.418213</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.458194</td>\n",
       "      <td>0.006362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.415039</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.456037</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.411767</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.454179</td>\n",
       "      <td>0.006773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.408796</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.452645</td>\n",
       "      <td>0.006817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.406101</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.451242</td>\n",
       "      <td>0.006920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.403640</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.450134</td>\n",
       "      <td>0.007257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.401330</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.449006</td>\n",
       "      <td>0.007261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.399186</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.448075</td>\n",
       "      <td>0.007055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.397473</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.447271</td>\n",
       "      <td>0.006895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.395124</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.446580</td>\n",
       "      <td>0.007197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.393255</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.445980</td>\n",
       "      <td>0.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.391638</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.445289</td>\n",
       "      <td>0.006869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.389519</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.444543</td>\n",
       "      <td>0.007214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.387902</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.443919</td>\n",
       "      <td>0.007211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.386293</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.443334</td>\n",
       "      <td>0.007276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.384898</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.443063</td>\n",
       "      <td>0.007114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.383515</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.442701</td>\n",
       "      <td>0.007168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.382033</td>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.442394</td>\n",
       "      <td>0.007045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.380568</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.442202</td>\n",
       "      <td>0.007103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.378937</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.442010</td>\n",
       "      <td>0.006949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.377805</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.441866</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.376774</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.441821</td>\n",
       "      <td>0.007164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.375338</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.441691</td>\n",
       "      <td>0.007098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.373886</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.441513</td>\n",
       "      <td>0.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>0.441298</td>\n",
       "      <td>0.007363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.371052</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.441177</td>\n",
       "      <td>0.007339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.369835</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.007275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.368736</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.440828</td>\n",
       "      <td>0.007231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.367631</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.440710</td>\n",
       "      <td>0.007417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.366548</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.440783</td>\n",
       "      <td>0.007515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.364989</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>0.440608</td>\n",
       "      <td>0.007490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.364071</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.440481</td>\n",
       "      <td>0.007633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.362835</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>0.007512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-logloss-mean  train-logloss-std  test-logloss-mean  test-logloss-std\n",
       "0             0.654580           0.000536           0.656680          0.001123\n",
       "1             0.622795           0.001050           0.626756          0.001573\n",
       "2             0.595998           0.001334           0.601964          0.002194\n",
       "3             0.573223           0.001615           0.581153          0.002629\n",
       "4             0.553685           0.001801           0.563505          0.003030\n",
       "5             0.536679           0.002084           0.548681          0.002879\n",
       "6             0.521496           0.002180           0.535619          0.003470\n",
       "7             0.508277           0.002232           0.524311          0.003450\n",
       "8             0.496489           0.002388           0.514817          0.003736\n",
       "9             0.486039           0.002441           0.506007          0.004347\n",
       "10            0.476188           0.002317           0.498485          0.004358\n",
       "11            0.467736           0.002362           0.492051          0.004494\n",
       "12            0.459941           0.002631           0.485878          0.004730\n",
       "13            0.452733           0.002663           0.480734          0.005268\n",
       "14            0.446083           0.002386           0.476177          0.005645\n",
       "15            0.439996           0.002137           0.472176          0.005912\n",
       "16            0.434987           0.002204           0.468421          0.005918\n",
       "17            0.430151           0.002098           0.465186          0.006108\n",
       "18            0.425851           0.002479           0.462419          0.006131\n",
       "19            0.421787           0.001859           0.460107          0.006218\n",
       "20            0.418213           0.002086           0.458194          0.006362\n",
       "21            0.415039           0.001745           0.456037          0.006507\n",
       "22            0.411767           0.001462           0.454179          0.006773\n",
       "23            0.408796           0.001529           0.452645          0.006817\n",
       "24            0.406101           0.001889           0.451242          0.006920\n",
       "25            0.403640           0.001661           0.450134          0.007257\n",
       "26            0.401330           0.001794           0.449006          0.007261\n",
       "27            0.399186           0.001737           0.448075          0.007055\n",
       "28            0.397473           0.001708           0.447271          0.006895\n",
       "29            0.395124           0.001650           0.446580          0.007197\n",
       "30            0.393255           0.001720           0.445980          0.007071\n",
       "31            0.391638           0.001846           0.445289          0.006869\n",
       "32            0.389519           0.001819           0.444543          0.007214\n",
       "33            0.387902           0.001599           0.443919          0.007211\n",
       "34            0.386293           0.001772           0.443334          0.007276\n",
       "35            0.384898           0.001906           0.443063          0.007114\n",
       "36            0.383515           0.001942           0.442701          0.007168\n",
       "37            0.382033           0.002091           0.442394          0.007045\n",
       "38            0.380568           0.002457           0.442202          0.007103\n",
       "39            0.378937           0.002183           0.442010          0.006949\n",
       "40            0.377805           0.002096           0.441866          0.007100\n",
       "41            0.376774           0.002180           0.441821          0.007164\n",
       "42            0.375338           0.002511           0.441691          0.007098\n",
       "43            0.373886           0.002789           0.441513          0.007087\n",
       "44            0.372600           0.002556           0.441298          0.007363\n",
       "45            0.371052           0.002638           0.441177          0.007339\n",
       "46            0.369835           0.002583           0.440945          0.007275\n",
       "47            0.368736           0.003091           0.440828          0.007231\n",
       "48            0.367631           0.003054           0.440710          0.007417\n",
       "49            0.366548           0.003491           0.440783          0.007515\n",
       "50            0.364989           0.003761           0.440608          0.007490\n",
       "51            0.364071           0.003726           0.440481          0.007633\n",
       "52            0.362835           0.003490           0.440476          0.007512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "}\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    xgb.DMatrix(train_test.iloc[:df_train.shape[0]], df_train['is_duplicate'].values),\n",
    "    num_boost_round=100,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
